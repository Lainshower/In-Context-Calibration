{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You need to download updated version for the MR datasets\n",
    "#!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/user122b/.cache/huggingface/datasets/mattymchen___parquet/mattymchen--mr-1c3e14b87ea7589a/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d378dcff47419ea888426e8772febc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('mattymchen/mr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [item['text'] for item in dataset['test']]\n",
    "labels = [item['label'] for item in dataset['test']]\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"negative\",\n",
    "            1:\"positive\",\n",
    "        }\n",
    "\n",
    "\n",
    "data_path = f'data/mr'\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_train, labels_train)):\n",
    "    item = {\n",
    "        \"Review\": text,\n",
    "        \"Sentiment\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'train.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_test, labels_test)):\n",
    "    item = {\n",
    "        \"Review\": text,\n",
    "        \"Sentiment\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'test.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/user122b/.cache/huggingface/datasets/SetFit___json/SetFit--CR-10c662058e6e2dc5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbd969a7a664c45968c8b7bdca35988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('SetFit/CR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'label_text'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (split, dataset) in enumerate(zip(['train', 'test'],[train_set, test_set])):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        item = {\n",
    "            \"Review\": datum['text'],\n",
    "            \"Sentiment\": datum['label_text'],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/cr'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fd9793bdf24510b14b65fbb6dd5deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/248 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/SetFit--subj to /home/user122b/.cache/huggingface/datasets/SetFit___json/SetFit--subj-15fb6571305f6f49/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b60ff7c68446c79dd9256737f70eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc2f4c61a1e4967837195e41d78590e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dfcc8d67964410b028987b3becacc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/364k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb95e5a66e745ebbfe8c993b44cc84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd864fdf142437cb7a429f6c2a96c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baaf4ebb93c646d4932697e22d86fbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/user122b/.cache/huggingface/datasets/SetFit___json/SetFit--subj-15fb6571305f6f49/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f24a2c9e4d4ac7825fd8c44e78eae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('SetFit/subj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (split, dataset) in enumerate(zip(['train', 'test'],[train_set, test_set])):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        item = {\n",
    "            \"Input\": datum['text'],\n",
    "            \"Label\": datum['label_text'],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/subj'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### poem sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset poem_sentiment (/home/user122b/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/f4990808f049126bcea572bba70613313212cd45f3b12a3e5586135e2de42f56)\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('poem_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'verse_text', 'label'],\n",
       "        num_rows: 892\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'verse_text', 'label'],\n",
       "        num_rows: 105\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'verse_text', 'label'],\n",
       "        num_rows: 104\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_labels(example):\n",
    "    # We'll keep examples where label is not 2 or 3\n",
    "    return example['label'] not in [3]\n",
    "\n",
    "dataset = dataset.filter(filter_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "valid_set = dataset['validation']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function filter_labels at 0x7f68ab3aaee0> of the transform datasets.arrow_dataset.Dataset.filter couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8acbcbba1ca4c2d881abce8e6c49161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b08e66c6f4d4280a4c25ae0ef9444e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e639710de14a4e669eb164e8affa7767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = train_set.filter(filter_labels)\n",
    "valid_set = valid_set.filter(filter_labels)\n",
    "test_set = test_set.filter(filter_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"negative\",\n",
    "            1:\"positive\",\n",
    "            2:\"no_impact\",\n",
    "            #3:\"mixed\", # there is no `mixed` on the test set\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'verse_text', 'label'],\n",
       "    num_rows: 843\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        item = {\n",
    "            \"Verse text\": datum['verse_text'],\n",
    "            \"Sentiment\": label_dict[datum['label']],\n",
    "            \"idx\": datum['id']\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/poem-sentiment'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/user122b/.cache/huggingface/datasets/glue/wnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('glue','wnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "valid_set = dataset['validation']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"not_entailment\",\n",
    "            1:\"entailment\",\n",
    "            -1:\"not_identified\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        item = {\n",
    "            \"premise\": datum['sentence1'],\n",
    "            \"hypothesis\": datum['sentence2'],\n",
    "            \"label\": label_dict[datum['label']],\n",
    "            \"idx\": datum['idx']\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/wnli'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset sick (/home/user122b/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('sick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'sentence_A', 'sentence_B', 'label', 'relatedness_score', 'entailment_AB', 'entailment_BA', 'sentence_A_original', 'sentence_B_original', 'sentence_A_dataset', 'sentence_B_dataset'],\n",
       "        num_rows: 4439\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'sentence_A', 'sentence_B', 'label', 'relatedness_score', 'entailment_AB', 'entailment_BA', 'sentence_A_original', 'sentence_B_original', 'sentence_A_dataset', 'sentence_B_dataset'],\n",
       "        num_rows: 495\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'sentence_A', 'sentence_B', 'label', 'relatedness_score', 'entailment_AB', 'entailment_BA', 'sentence_A_original', 'sentence_B_original', 'sentence_A_dataset', 'sentence_B_dataset'],\n",
       "        num_rows: 4906\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "valid_set = dataset['validation']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"entailment\",\n",
    "            1:\"neutral\",\n",
    "            2:\"contradiction\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        item = {\n",
    "            \"premise\": datum['sentence_A'],\n",
    "            \"hypothesis\": datum['sentence_B'],\n",
    "            \"label\": label_dict[datum['label']],\n",
    "            \"idx\": datum['id']\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/sick'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tweet hate | offensive | irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/home/user122b/.cache/huggingface/datasets/tweet_eval/hate/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n",
      "Reusing dataset tweet_eval (/home/user122b/.cache/huggingface/datasets/tweet_eval/irony/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n",
      "Reusing dataset tweet_eval (/home/user122b/.cache/huggingface/datasets/tweet_eval/offensive/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n"
     ]
    }
   ],
   "source": [
    "for subset_name  in ['hate', 'irony', 'offensive']:\n",
    "    if subset_name == \"hate\":\n",
    "        label_dict = {\n",
    "                0:\"neutral\",\n",
    "                1:\"hate\",\n",
    "            }\n",
    "    elif subset_name == \"irony\":\n",
    "        label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"ironic\",\n",
    "        }\n",
    "    elif subset_name == \"offensive\":\n",
    "        label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"offensive\",\n",
    "        }\n",
    "    dataset = load_dataset('tweet_eval', subset_name)\n",
    "\n",
    "    train_set = dataset['train']\n",
    "    valid_set = dataset['validation']\n",
    "    test_set = dataset['test']\n",
    "\n",
    "    for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "        data = []\n",
    "        for idx, datum in enumerate(dataset):\n",
    "            item = {\n",
    "                \"Tweet\": datum['text'],\n",
    "                \"Label\": label_dict[datum['label']],\n",
    "                \"idx\": idx\n",
    "            }\n",
    "            data.append(item)\n",
    "\n",
    "        # Write to jsonl file\n",
    "        data_path = f'data/tweet-{subset_name}'\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet atheism & feminist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/home/user122b/.cache/huggingface/datasets/tweet_eval/stance_atheism/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n",
      "Reusing dataset tweet_eval (/home/user122b/.cache/huggingface/datasets/tweet_eval/stance_feminist/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n"
     ]
    }
   ],
   "source": [
    "for subset_name  in ['stance_atheism', 'stance_feminist']:\n",
    "    label_dict = {\n",
    "        0:\"none\",\n",
    "        1:\"against\",\n",
    "        2:\"favor\"\n",
    "    }\n",
    "    dataset = load_dataset('tweet_eval', subset_name)\n",
    "\n",
    "    train_set = dataset['train']\n",
    "    valid_set = dataset['validation']\n",
    "    test_set = dataset['test']\n",
    "\n",
    "    for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "        data = []\n",
    "        for idx, datum in enumerate(dataset):\n",
    "            item = {\n",
    "                \"Tweet\": datum['text'],\n",
    "                \"Label\": label_dict[datum['label']],\n",
    "                \"idx\": idx\n",
    "            }\n",
    "            data.append(item)\n",
    "\n",
    "        # Write to jsonl file\n",
    "        data_path = f'data/tweet-{subset_name}'\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hate_speech18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset hate_speech18 (/home/user122b/.cache/huggingface/datasets/hate_speech18/default/0.0.0/721493a602047e18cca4c8ddfab5c04c78c7ad6df2dca028913efef2e5bb1ce9)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('hate_speech18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb1252a58ca4e1eb13263e65be38a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_labels(example):\n",
    "    # We'll keep examples where label is not 2 or 3\n",
    "    return example['label'] not in [2, 3]\n",
    "\n",
    "dataset = dataset.filter(filter_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [item['text'] for item in dataset['train']]\n",
    "labels = [item['label'] for item in dataset['train']]\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"hate\",\n",
    "        }\n",
    "\n",
    "\n",
    "data_path = f'data/hate-speech18'\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_train, labels_train)):\n",
    "    item = {\n",
    "        \"Text\": text,\n",
    "        \"Label\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'train.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_test, labels_test)):\n",
    "    item = {\n",
    "        \"Text\": text,\n",
    "        \"Label\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'test.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethos Binary | Ethos | Religion | National Origion | Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ethos (/home/user122b/.cache/huggingface/datasets/ethos/binary/1.0.0/acfabdb84989bd22ea8f7d11169a3eaba8b807b44fea201ce2a1820f92963ff3)\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ethos', 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 998\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [item for item in dataset['train'] if len(item['text']) <= 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [item['text'] for item in dataset]\n",
    "labels = [item['label'] for item in dataset]\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"hate\",\n",
    "        }\n",
    "\n",
    "\n",
    "data_path = f'data/ethos-binary'\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_train, labels_train)):\n",
    "    item = {\n",
    "        \"Text\": text,\n",
    "        \"Label\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'train.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_test, labels_test)):\n",
    "    item = {\n",
    "        \"Text\": text,\n",
    "        \"Label\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'test.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethos Race | Nation | Religion | Nation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ethos (/home/user122b/.cache/huggingface/datasets/ethos/multilabel/1.0.0/acfabdb84989bd22ea8f7d11169a3eaba8b807b44fea201ce2a1820f92963ff3)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ethos', 'multilabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'violence', 'directed_vs_generalized', 'gender', 'race', 'national_origin', 'disability', 'religion', 'sexual_orientation'],\n",
       "        num_rows: 433\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [item for item in dataset['train'] if len(item['text']) <= 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in ['race', 'religion', 'national_origin']:\n",
    "    texts = [item['text'] for item in dataset]\n",
    "    labels = [item[subset] for item in dataset]\n",
    "    texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "    \n",
    "    label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"hate\",\n",
    "        }\n",
    "\n",
    "\n",
    "    data_path = f'data/ethos-{subset}'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "    data = []\n",
    "    for idx, (text, label) in enumerate(zip(texts_train, labels_train)):\n",
    "        item = {\n",
    "            \"Text\": text,\n",
    "            \"Label\": label_dict[label],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    with open(os.path.join(data_path, 'train.jsonl'), 'w') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "    data = []\n",
    "    for idx, (text, label) in enumerate(zip(texts_test, labels_test)):\n",
    "        item = {\n",
    "            \"Text\": text,\n",
    "            \"Label\": label_dict[label],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    with open(os.path.join(data_path, 'test.jsonl'), 'w') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 45000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
