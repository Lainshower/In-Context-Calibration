{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You need to download updated version for the MR datasets\n",
    "#!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/user122b/.cache/huggingface/datasets/mattymchen___parquet/mattymchen--mr-1c3e14b87ea7589a/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d378dcff47419ea888426e8772febc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('mattymchen/mr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [item['text'] for item in dataset['test']]\n",
    "labels = [item['label'] for item in dataset['test']]\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"negative\",\n",
    "            1:\"positive\",\n",
    "        }\n",
    "\n",
    "\n",
    "data_path = f'data/mr'\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_train, labels_train)):\n",
    "    item = {\n",
    "        \"Review\": text,\n",
    "        \"Sentiment\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'train.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_test, labels_test)):\n",
    "    item = {\n",
    "        \"Review\": text,\n",
    "        \"Sentiment\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'test.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/user122b/.cache/huggingface/datasets/SetFit___json/SetFit--CR-10c662058e6e2dc5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbd969a7a664c45968c8b7bdca35988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('SetFit/CR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'label_text'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (split, dataset) in enumerate(zip(['train', 'test'],[train_set, test_set])):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        item = {\n",
    "            \"Review\": datum['text'],\n",
    "            \"Sentiment\": datum['label_text'],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/cr'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fd9793bdf24510b14b65fbb6dd5deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/248 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/SetFit--subj to /home/user122b/.cache/huggingface/datasets/SetFit___json/SetFit--subj-15fb6571305f6f49/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b60ff7c68446c79dd9256737f70eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc2f4c61a1e4967837195e41d78590e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dfcc8d67964410b028987b3becacc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/364k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb95e5a66e745ebbfe8c993b44cc84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd864fdf142437cb7a429f6c2a96c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baaf4ebb93c646d4932697e22d86fbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/user122b/.cache/huggingface/datasets/SetFit___json/SetFit--subj-15fb6571305f6f49/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f24a2c9e4d4ac7825fd8c44e78eae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('SetFit/subj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (split, dataset) in enumerate(zip(['train', 'test'],[train_set, test_set])):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        item = {\n",
    "            \"Input\": datum['text'],\n",
    "            \"Label\": datum['label_text'],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/subj'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### poem sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset poem_sentiment (/home/user122b/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/f4990808f049126bcea572bba70613313212cd45f3b12a3e5586135e2de42f56)\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('poem_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'verse_text', 'label'],\n",
       "        num_rows: 892\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'verse_text', 'label'],\n",
       "        num_rows: 105\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'verse_text', 'label'],\n",
       "        num_rows: 104\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_labels(example):\n",
    "    # We'll keep examples where label is not 2 or 3\n",
    "    return example['label'] not in [3]\n",
    "\n",
    "dataset = dataset.filter(filter_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "valid_set = dataset['validation']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function filter_labels at 0x7f68ab3aaee0> of the transform datasets.arrow_dataset.Dataset.filter couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8acbcbba1ca4c2d881abce8e6c49161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b08e66c6f4d4280a4c25ae0ef9444e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e639710de14a4e669eb164e8affa7767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = train_set.filter(filter_labels)\n",
    "valid_set = valid_set.filter(filter_labels)\n",
    "test_set = test_set.filter(filter_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"negative\",\n",
    "            1:\"positive\",\n",
    "            2:\"no_impact\",\n",
    "            #3:\"mixed\", # there is no `mixed` on the test set\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'verse_text', 'label'],\n",
       "    num_rows: 843\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        item = {\n",
    "            \"Verse text\": datum['verse_text'],\n",
    "            \"Sentiment\": label_dict[datum['label']],\n",
    "            \"idx\": datum['id']\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/poem-sentiment'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/user122b/.cache/huggingface/datasets/glue/wnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('glue','wnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "valid_set = dataset['validation']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"not_entailment\",\n",
    "            1:\"entailment\",\n",
    "            -1:\"not_identified\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        item = {\n",
    "            \"premise\": datum['sentence1'],\n",
    "            \"hypothesis\": datum['sentence2'],\n",
    "            \"label\": label_dict[datum['label']],\n",
    "            \"idx\": datum['idx']\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/wnli'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e-snli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d20b57eb8c8429fbfede4d0d9796b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4f50c0dcef4d4c8b6d6daa6adb8890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a82bf2acc74b0baba276f4e4c9f168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset esnli/plain_text to /home/user122b/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2699085082be44b3abb55d497b198421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1264a224a15d4137a71afd9f5bd46621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e15f0e3bf445f493e74e36db8354c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/549367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0dc27d4030424c87432f46ab8d2734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/9842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d432e9432d94356bab40357cc4e7bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset esnli downloaded and prepared to /home/user122b/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbb8fcaf7414ba6bb6eac326e3f2ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('esnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "valid_set = dataset['validation']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],\n",
       "    num_rows: 549367\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"entailment\",\n",
    "            1:\"neutral\",\n",
    "            2:\"contradiction\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (split, dataset) in enumerate(zip(['train', 'val', 'test'],[train_set, valid_set, test_set])):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        item = {\n",
    "            \"premise\": datum['premise'],\n",
    "            \"hypothesis\": datum['hypothesis'],\n",
    "            \"explantion\": datum['explanation_1'],\n",
    "            \"label\": label_dict[datum['label']],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/esnli'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset sick (/home/user122b/.cache/huggingface/datasets/sick/default/0.0.0/c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('sick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'sentence_A', 'sentence_B', 'label', 'relatedness_score', 'entailment_AB', 'entailment_BA', 'sentence_A_original', 'sentence_B_original', 'sentence_A_dataset', 'sentence_B_dataset'],\n",
       "        num_rows: 4439\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'sentence_A', 'sentence_B', 'label', 'relatedness_score', 'entailment_AB', 'entailment_BA', 'sentence_A_original', 'sentence_B_original', 'sentence_A_dataset', 'sentence_B_dataset'],\n",
       "        num_rows: 495\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'sentence_A', 'sentence_B', 'label', 'relatedness_score', 'entailment_AB', 'entailment_BA', 'sentence_A_original', 'sentence_B_original', 'sentence_A_dataset', 'sentence_B_dataset'],\n",
       "        num_rows: 4906\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "valid_set = dataset['validation']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"entailment\",\n",
    "            1:\"neutral\",\n",
    "            2:\"contradiction\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        item = {\n",
    "            \"premise\": datum['sentence_A'],\n",
    "            \"hypothesis\": datum['sentence_B'],\n",
    "            \"label\": label_dict[datum['label']],\n",
    "            \"idx\": datum['id']\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/sick'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tweet hate | offensive | irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/home/user122b/.cache/huggingface/datasets/tweet_eval/hate/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n",
      "Reusing dataset tweet_eval (/home/user122b/.cache/huggingface/datasets/tweet_eval/irony/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n",
      "Reusing dataset tweet_eval (/home/user122b/.cache/huggingface/datasets/tweet_eval/offensive/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n"
     ]
    }
   ],
   "source": [
    "for subset_name  in ['hate', 'irony', 'offensive']:\n",
    "    if subset_name == \"hate\":\n",
    "        label_dict = {\n",
    "                0:\"neutral\",\n",
    "                1:\"hate\",\n",
    "            }\n",
    "    elif subset_name == \"irony\":\n",
    "        label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"ironic\",\n",
    "        }\n",
    "    elif subset_name == \"offensive\":\n",
    "        label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"offensive\",\n",
    "        }\n",
    "    dataset = load_dataset('tweet_eval', subset_name)\n",
    "\n",
    "    train_set = dataset['train']\n",
    "    valid_set = dataset['validation']\n",
    "    test_set = dataset['test']\n",
    "\n",
    "    for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "        data = []\n",
    "        for idx, datum in enumerate(dataset):\n",
    "            item = {\n",
    "                \"Tweet\": datum['text'],\n",
    "                \"Label\": label_dict[datum['label']],\n",
    "                \"idx\": idx\n",
    "            }\n",
    "            data.append(item)\n",
    "\n",
    "        # Write to jsonl file\n",
    "        data_path = f'data/tweet-{subset_name}'\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet atheism & feminist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/home/user122b/.cache/huggingface/datasets/tweet_eval/stance_atheism/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n",
      "Reusing dataset tweet_eval (/home/user122b/.cache/huggingface/datasets/tweet_eval/stance_feminist/1.1.0/79e21f7659e902ea14f624232219492d972fe5e0f9d8c94363acc7f916a6be48)\n"
     ]
    }
   ],
   "source": [
    "for subset_name  in ['stance_atheism', 'stance_feminist']:\n",
    "    label_dict = {\n",
    "        0:\"none\",\n",
    "        1:\"against\",\n",
    "        2:\"favor\"\n",
    "    }\n",
    "    dataset = load_dataset('tweet_eval', subset_name)\n",
    "\n",
    "    train_set = dataset['train']\n",
    "    valid_set = dataset['validation']\n",
    "    test_set = dataset['test']\n",
    "\n",
    "    for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "        data = []\n",
    "        for idx, datum in enumerate(dataset):\n",
    "            item = {\n",
    "                \"Tweet\": datum['text'],\n",
    "                \"Label\": label_dict[datum['label']],\n",
    "                \"idx\": idx\n",
    "            }\n",
    "            data.append(item)\n",
    "\n",
    "        # Write to jsonl file\n",
    "        data_path = f'data/tweet-{subset_name}'\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "        with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hate_speech18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset hate_speech18 (/home/user122b/.cache/huggingface/datasets/hate_speech18/default/0.0.0/721493a602047e18cca4c8ddfab5c04c78c7ad6df2dca028913efef2e5bb1ce9)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('hate_speech18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function filter_labels at 0x7f7dc0de2310> of the transform datasets.arrow_dataset.Dataset.filter couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cde74fb6494980b01db67b90d953ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_labels(example):\n",
    "    # We'll keep examples where label is not 2 or 3\n",
    "    return example['label'] not in [2, 3]\n",
    "\n",
    "dataset = dataset.filter(filter_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [item['text'] for item in dataset['train']]\n",
    "labels = [item['label'] for item in dataset['train']]\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"hate\",\n",
    "        }\n",
    "\n",
    "\n",
    "data_path = f'data/hate-speech18'\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_train, labels_train)):\n",
    "    item = {\n",
    "        \"Text\": text,\n",
    "        \"Label\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'train.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_test, labels_test)):\n",
    "    item = {\n",
    "        \"Text\": text,\n",
    "        \"Label\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'test.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethos Binary | Ethos | Religion | National Origion | Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ethos (/home/user122b/.cache/huggingface/datasets/ethos/binary/1.0.0/acfabdb84989bd22ea8f7d11169a3eaba8b807b44fea201ce2a1820f92963ff3)\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ethos', 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 998\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [item for item in dataset['train'] if len(item['text']) <= 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [item['text'] for item in dataset]\n",
    "labels = [item['label'] for item in dataset]\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"hate\",\n",
    "        }\n",
    "\n",
    "\n",
    "data_path = f'data/ethos-binary'\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_train, labels_train)):\n",
    "    item = {\n",
    "        \"Text\": text,\n",
    "        \"Label\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'train.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_test, labels_test)):\n",
    "    item = {\n",
    "        \"Text\": text,\n",
    "        \"Label\": label_dict[label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'test.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethos Race | Nation | Religion | Nation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ethos (/home/user122b/.cache/huggingface/datasets/ethos/multilabel/1.0.0/acfabdb84989bd22ea8f7d11169a3eaba8b807b44fea201ce2a1820f92963ff3)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ethos', 'multilabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'violence', 'directed_vs_generalized', 'gender', 'race', 'national_origin', 'disability', 'religion', 'sexual_orientation'],\n",
       "        num_rows: 433\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [item for item in dataset['train'] if len(item['text']) <= 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in ['race', 'religion', 'national_origin']:\n",
    "    texts = [item['text'] for item in dataset]\n",
    "    labels = [item[subset] for item in dataset]\n",
    "    texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "    \n",
    "    label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"hate\",\n",
    "        }\n",
    "\n",
    "\n",
    "    data_path = f'data/ethos-{subset}'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "    data = []\n",
    "    for idx, (text, label) in enumerate(zip(texts_train, labels_train)):\n",
    "        item = {\n",
    "            \"Text\": text,\n",
    "            \"Label\": label_dict[label],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    with open(os.path.join(data_path, 'train.jsonl'), 'w') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "    data = []\n",
    "    for idx, (text, label) in enumerate(zip(texts_test, labels_test)):\n",
    "        item = {\n",
    "            \"Text\": text,\n",
    "            \"Label\": label_dict[label],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    with open(os.path.join(data_path, 'test.jsonl'), 'w') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in ['gender', 'violence', 'disability']:\n",
    "    texts = [item['text'] for item in dataset]\n",
    "    labels = [item[subset] for item in dataset]\n",
    "    texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "    \n",
    "    label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"hate\",\n",
    "        }\n",
    "\n",
    "\n",
    "    data_path = f'data/ethos-{subset}'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "    data = []\n",
    "    for idx, (text, label) in enumerate(zip(texts_train, labels_train)):\n",
    "        item = {\n",
    "            \"Text\": text,\n",
    "            \"Label\": label_dict[label],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    with open(os.path.join(data_path, 'train.jsonl'), 'w') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "    data = []\n",
    "    for idx, (text, label) in enumerate(zip(texts_test, labels_test)):\n",
    "        item = {\n",
    "            \"Text\": text,\n",
    "            \"Label\": label_dict[label],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    with open(os.path.join(data_path, 'test.jsonl'), 'w') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 45000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Civil Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda6ac8d7e4840658ad629020adc9b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865c18195c1d4f1f886ed13eff9b1742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset civil_comments/default to /home/user122b/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f019cf5a98df4434af59792a917a0f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1804874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008191120eeb4e12a5109856a26fe96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/97320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa82e756e764454583fa797aaa4e3700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/97320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset civil_comments downloaded and prepared to /home/user122b/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edfafc3227c489480b473a71968b9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('civil_comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
       "        num_rows: 1804874\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
       "        num_rows: 97320\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
       "        num_rows: 97320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "valid_set = dataset['validation']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"toxic\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "    data = []\n",
    "    for idx, datum in enumerate(dataset):\n",
    "        if datum['toxicity'] >= 0.5:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        item = {\n",
    "            \"Text\": datum['text'],\n",
    "            \"Toxicity\": label_dict[label],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/civil_comments'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0171293c2c05430cb242ba08d84d14e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/28.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b8511459de4d749c78512ebc907ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbbd8beb1654e7e80ac0462a3e1635c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/27.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mnli to /home/user122b/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f94d96d24041968006680ab552d13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee77826ee53e4d7ea7084c07867b546c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75c682ca3f54aa6a663e97833e425a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c71d7b14ee44f8680d86a1b3035d5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_matched split:   0%|          | 0/9796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82d11390d3942608c7d3b635f5680db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_mismatched split:   0%|          | 0/9847 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/user122b/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280b1cfd9d9e4431bd11ceccfc7c65c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('glue','mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets\n",
    "train_set = dataset['train']\n",
    "valid_set = dataset['validation_matched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"entailment\",\n",
    "            1:\"not_entailment\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in zip(['train', 'val'],[train_set, valid_set]):\n",
    "    data = []\n",
    "    for datum in dataset:\n",
    "        if datum['label'] == 0:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        item = {\n",
    "            \"premise\": datum['premise'],\n",
    "            \"hypothesis\": datum['hypothesis'],\n",
    "            \"label\": label_dict[label],\n",
    "            \"idx\": datum['idx']\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/mnli'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gutenberg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset gutenberg_time (/home/user122b/.cache/huggingface/datasets/gutenberg_time/gutenberg/0.0.0/07dc8573d6c1b4e90763a00d93757888c9ca40676d4c585e0e5800d81307c0f9)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('gutenberg_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Consider IF OOM ERROR\n",
    "# dataset = [item for item in dataset['train'] if len(item['tok_context']) <= 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['guten_id', 'hour_reference', 'time_phrase', 'is_ambiguous', 'time_pos_start', 'time_pos_end', 'tok_context'],\n",
       "        num_rows: 120694\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [item['tok_context'] for item in dataset['train']]\n",
    "labels = [str(item['hour_reference']) for item in dataset['train']]\n",
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    0: \"zero\",\n",
    "    1: \"one\",\n",
    "    2: \"two\",\n",
    "    3: \"three\",\n",
    "    4: \"four\",\n",
    "    5: \"five\",\n",
    "    6: \"six\",\n",
    "    7: \"seven\",\n",
    "    8: \"eight\",\n",
    "    9: \"nine\",\n",
    "    10: \"ten\",\n",
    "    11: \"eleven\",\n",
    "    12: \"twelve\",\n",
    "    13: \"thirteen\",\n",
    "    14: \"fourteen\",\n",
    "    15: \"fifteen\",\n",
    "    16: \"sixteen\",\n",
    "    17: \"seventeen\",\n",
    "    18: \"eighteen\",\n",
    "    19: \"nineteen\",\n",
    "    20: \"twenty\",\n",
    "    21: \"twenty_one\",\n",
    "    22: \"twenty_two\",\n",
    "    23: \"twenty_three\"\n",
    "}    \n",
    "\n",
    "\n",
    "data_path = f'data/hate-speech18'\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_train, labels_train)):\n",
    "    item = {\n",
    "        \"Text\": text,\n",
    "        \"Label\": label_dict[int(label)],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'train.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "data = []\n",
    "for idx, (text, label) in enumerate(zip(texts_test, labels_test)):\n",
    "    item = {\n",
    "        \"Text\": text,\n",
    "        \"Label\": label_dict[int(label],\n",
    "        \"idx\": idx\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Write to jsonl file\n",
    "with open(os.path.join(data_path, 'test.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social Bias Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset social_bias_frames (/home/user122b/.cache/huggingface/datasets/social_bias_frames/default/0.0.0/79706db13a32c7f9614b997cc4326cbda14e6d3968892a3f0e76c4a970e7e510)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d8a0e1b1c1418eabc784b47c8cb7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('social_bias_frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['whoTarget', 'intentYN', 'sexYN', 'sexReason', 'offensiveYN', 'annotatorGender', 'annotatorMinority', 'sexPhrase', 'speakerMinorityYN', 'WorkerId', 'HITId', 'annotatorPolitics', 'annotatorRace', 'annotatorAge', 'post', 'targetMinority', 'targetCategory', 'targetStereotype', 'dataSource'],\n",
       "        num_rows: 17501\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['whoTarget', 'intentYN', 'sexYN', 'sexReason', 'offensiveYN', 'annotatorGender', 'annotatorMinority', 'sexPhrase', 'speakerMinorityYN', 'WorkerId', 'HITId', 'annotatorPolitics', 'annotatorRace', 'annotatorAge', 'post', 'targetMinority', 'targetCategory', 'targetStereotype', 'dataSource'],\n",
       "        num_rows: 16738\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['whoTarget', 'intentYN', 'sexYN', 'sexReason', 'offensiveYN', 'annotatorGender', 'annotatorMinority', 'sexPhrase', 'speakerMinorityYN', 'WorkerId', 'HITId', 'annotatorPolitics', 'annotatorRace', 'annotatorAge', 'post', 'targetMinority', 'targetCategory', 'targetStereotype', 'dataSource'],\n",
       "        num_rows: 112900\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Assuming `dataset` is your Hugging Face dataset\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "# Select 'post' and 'offensiveYN' columns\n",
    "train_df = train_df[['post', 'offensiveYN']]\n",
    "# Drop duplicates based on 'post' column\n",
    "train_df = train_df.drop_duplicates(subset='post')\n",
    "# If you need to convert it back to a Hugging Face dataset:\n",
    "train_set = Dataset.from_pandas(train_df)\n",
    "\n",
    "# Assuming `dataset` is your Hugging Face dataset\n",
    "valid_df = pd.DataFrame(dataset['validation'])\n",
    "# Select 'post' and 'offensiveYN' columns\n",
    "valid_df = valid_df[['post', 'offensiveYN']]\n",
    "# Drop duplicates based on 'post' column\n",
    "valid_df = valid_df.drop_duplicates(subset='post')\n",
    "# If you need to convert it back to a Hugging Face dataset:\n",
    "valid_set = Dataset.from_pandas(valid_df)\n",
    "\n",
    "# Assuming `dataset` is your Hugging Face dataset\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "# Select 'post' and 'offensiveYN' columns\n",
    "test_df = test_df[['post', 'offensiveYN']]\n",
    "# Drop duplicates based on 'post' column\n",
    "test_df = test_df.drop_duplicates(subset='post')\n",
    "# If you need to convert it back to a Hugging Face dataset:\n",
    "test_set = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['offensiveYN'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "            0:\"neutral\",\n",
    "            1:\"offensive\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dataset in zip(['train', 'val', 'test'],[train_set, valid_set, test_set]):\n",
    "    data = []\n",
    "    for idx, datum in enumerate(dataset):\n",
    "        if datum['offensiveYN'] == '0.5' or datum['offensiveYN'] == '1.0':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        item = {\n",
    "            \"Post\": datum['post'],\n",
    "            \"Offensive\": label_dict[label],\n",
    "            \"idx\": idx\n",
    "        }\n",
    "        data.append(item)\n",
    "\n",
    "    # Write to jsonl file\n",
    "    data_path = 'data/sbic'\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    with open(os.path.join(data_path,f'{split}.jsonl'), 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
